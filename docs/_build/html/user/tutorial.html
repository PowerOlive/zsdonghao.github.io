

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Tutorial &mdash; Hao Dong 1.1 documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  

  

  
    <link rel="top" title="Hao Dong 1.1 documentation" href="../index.html"/> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> Hao Dong
          

          
          </a>

          
            
            
              <div class="version">
                1.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <ul class="simple">
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../index.html">Hao Dong</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          

 



<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../index.html">Docs</a> &raquo;</li>
      
    <li>Tutorial</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/user/tutorial.txt" rel="nofollow"> View page source</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="tutorial">
<span id="id1"></span><h1>Tutorial<a class="headerlink" href="#tutorial" title="Permalink to this headline">¶</a></h1>
<p>For deep learning, this tutorial will walk you through building a handwritten
digits classifier using the MNIST dataset, arguably the &#8220;Hello World&#8221; of neural
networks. For reinforcement learning, we will let computer learns to play Pong
game from the original screen inputs. For nature language processing, we start
from word embedding, and then describe language modeling and machine
translation.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">For experts: Read the source code of <code class="docutils literal"><span class="pre">InputLayer</span></code> and <code class="docutils literal"><span class="pre">DenseLayer</span></code>, you
will understand how <a class="reference external" href="https://github.com/zsdonghao/tunelayer/">TuneLayer</a> work. After that, we recommend you to read
the codes for tutorial directly.</p>
</div>
<div class="section" id="before-we-start">
<h2>Before we start<a class="headerlink" href="#before-we-start" title="Permalink to this headline">¶</a></h2>
<p>The tutorial assumes that you are somewhat familiar with neural networks and
TensorFlow (the library which <a class="reference external" href="https://github.com/zsdonghao/tunelayer/">TuneLayer</a> is built on top of). You can try to learn
both at once from the <a class="reference external" href="http://deeplearning.stanford.edu/tutorial/">Deeplearning Tutorial</a>.</p>
<p>For a more slow-paced introduction to artificial neural networks, we recommend
<a class="reference external" href="http://cs231n.github.io/">Convolutional Neural Networks for Visual Recognition</a> by Andrej Karpathy et
al., <a class="reference external" href="http://neuralnetworksanddeeplearning.com/">Neural Networks and Deep Learning</a> by Michael Nielsen.</p>
<p>To learn more about TensorFlow, have a look at the <a class="reference external" href="https://www.tensorflow.org/versions/r0.9/tutorials/index.html">TensorFlow tutorial</a>. You will not
need all of it, but a basic understanding of how TensorFlow works is required to be
able to use <a class="reference external" href="https://github.com/zsdonghao/tunelayer/">TuneLayer</a>. If you&#8217;re new to TensorFlow, going through that tutorial.</p>
</div>
<div class="section" id="run-the-mnist-example">
<h2>Run the MNIST example<a class="headerlink" href="#run-the-mnist-example" title="Permalink to this headline">¶</a></h2>
<a class="reference internal image-reference" href="../_images/mnist.jpeg" id="fig-0601"><img alt="../_images/mnist.jpeg" class="align-center" id="fig-0601" src="../_images/mnist.jpeg" style="width: 2252.0px; height: 712.0px;" /></a>
<p>In the first part of the tutorial, we will just run the MNIST example that&#8217;s
included in the source distribution of <a class="reference external" href="https://github.com/zsdonghao/tunelayer/">TuneLayer</a>. MNIST dataset contains 60000
handwritten digits that is commonly used for training various
image processing systems, each of digit has 28x28 pixels.</p>
<p>We assume that you have already run through the <a class="reference internal" href="installation.html#installation"><span class="std std-ref">Installation</span></a>. If you
haven&#8217;t done so already, get a copy of the source tree of TuneLayer, and navigate
to the folder in a terminal window. Enter the folder and run the <code class="docutils literal"><span class="pre">tutorial_mnist.py</span></code>
example script:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>python tutorial_mnist.py
</pre></div>
</div>
<p>If everything is set up correctly, you will get an output like the following:</p>
<div class="highlight-text"><div class="highlight"><pre><span></span>TuneLayer: GPU MEM Fraction 0.300000
Downloading train-images-idx3-ubyte.gz
Downloading train-labels-idx1-ubyte.gz
Downloading t10k-images-idx3-ubyte.gz
Downloading t10k-labels-idx1-ubyte.gz

X_train.shape (50000, 784)
y_train.shape (50000,)
X_val.shape (10000, 784)
y_val.shape (10000,)
X_test.shape (10000, 784)
y_test.shape (10000,)
X float32   y int64

TuneLayer:Instantiate InputLayer input_layer (?, 784)
TuneLayer:Instantiate DropoutLayer drop1: keep: 0.800000
TuneLayer:Instantiate DenseLayer relu1: 800, &lt;function relu at 0x11281cb70&gt;
TuneLayer:Instantiate DropoutLayer drop2: keep: 0.500000
TuneLayer:Instantiate DenseLayer relu2: 800, &lt;function relu at 0x11281cb70&gt;
TuneLayer:Instantiate DropoutLayer drop3: keep: 0.500000
TuneLayer:Instantiate DenseLayer output_layer: 10, &lt;function identity at 0x115e099d8&gt;

param 0: (784, 800) (mean: -0.000053, median: -0.000043 std: 0.035558)
param 1: (800,) (mean: 0.000000, median: 0.000000 std: 0.000000)
param 2: (800, 800) (mean: 0.000008, median: 0.000041 std: 0.035371)
param 3: (800,) (mean: 0.000000, median: 0.000000 std: 0.000000)
param 4: (800, 10) (mean: 0.000469, median: 0.000432 std: 0.049895)
param 5: (10,) (mean: 0.000000, median: 0.000000 std: 0.000000)
num of params: 1276810

layer 0: Tensor(&quot;dropout/mul_1:0&quot;, shape=(?, 784), dtype=float32)
layer 1: Tensor(&quot;Relu:0&quot;, shape=(?, 800), dtype=float32)
layer 2: Tensor(&quot;dropout_1/mul_1:0&quot;, shape=(?, 800), dtype=float32)
layer 3: Tensor(&quot;Relu_1:0&quot;, shape=(?, 800), dtype=float32)
layer 4: Tensor(&quot;dropout_2/mul_1:0&quot;, shape=(?, 800), dtype=float32)
layer 5: Tensor(&quot;add_2:0&quot;, shape=(?, 10), dtype=float32)

learning_rate: 0.000100
batch_size: 128

Epoch 1 of 500 took 0.342539s
  train loss: 0.330111
  val loss: 0.298098
  val acc: 0.910700
Epoch 10 of 500 took 0.356471s
  train loss: 0.085225
  val loss: 0.097082
  val acc: 0.971700
Epoch 20 of 500 took 0.352137s
  train loss: 0.040741
  val loss: 0.070149
  val acc: 0.978600
Epoch 30 of 500 took 0.350814s
  train loss: 0.022995
  val loss: 0.060471
  val acc: 0.982800
Epoch 40 of 500 took 0.350996s
  train loss: 0.013713
  val loss: 0.055777
  val acc: 0.983700
...
</pre></div>
</div>
<p>The example script allows you to try different models, including Multi-Layer Perceptron,
Dropout, Dropconnect, Stacked Denoising Autoencoder and Convolutional Neural Network.
Select different models from <code class="docutils literal"><span class="pre">if</span> <span class="pre">__name__</span> <span class="pre">==</span> <span class="pre">'__main__':</span></code>.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">main_test_layers</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)</span>
<span class="n">main_test_denoise_AE</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)</span>
<span class="n">main_test_stacked_denoise_AE</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)</span>
<span class="n">main_test_cnn_layer</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="understand-the-mnist-example">
<h2>Understand the MNIST example<a class="headerlink" href="#understand-the-mnist-example" title="Permalink to this headline">¶</a></h2>
<p>Let&#8217;s now investigate what&#8217;s needed to make that happen! To follow along, open
up the source code.</p>
<div class="section" id="preface">
<h3>Preface<a class="headerlink" href="#preface" title="Permalink to this headline">¶</a></h3>
<p>The first thing you might notice is that besides TuneLayer, we also import numpy
and tensorflow:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tunelayer</span> <span class="kn">as</span> <span class="nn">tl</span>
<span class="kn">from</span> <span class="nn">tunelayer.layers</span> <span class="kn">import</span> <span class="n">set_keep</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">time</span>
</pre></div>
</div>
<p>As we know, TuneLayer is built on top of TensorFlow, it is meant as a supplement helping
with some tasks, not as a replacement. You will always mix TuneLayer with some
vanilla TensorFlow code. The <code class="docutils literal"><span class="pre">set_keep</span></code> is used to access the placeholder of keeping probabilities
when using Denoising Autoencoder.</p>
</div>
<div class="section" id="loading-data">
<h3>Loading data<a class="headerlink" href="#loading-data" title="Permalink to this headline">¶</a></h3>
<p>The first piece of code defines a function <code class="docutils literal"><span class="pre">load_mnist_dataset()</span></code>. Its purpose is
to download the MNIST dataset (if it hasn&#8217;t been downloaded yet) and return it
in the form of regular numpy arrays. There is no TuneLayer involved at all, so
for the purpose of this tutorial, we can regard it as:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> \
                  <span class="n">tl</span><span class="o">.</span><span class="n">files</span><span class="o">.</span><span class="n">load_mnist_dataset</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">784</span><span class="p">))</span>
</pre></div>
</div>
<p><code class="docutils literal"><span class="pre">X_train.shape</span></code> is <code class="docutils literal"><span class="pre">(50000,</span> <span class="pre">784)</span></code>, to be interpreted as: 50,000
images and each image has 784 pixels. <code class="docutils literal"><span class="pre">y_train.shape</span></code> is simply <code class="docutils literal"><span class="pre">(50000,)</span></code>, which is a vector the same
length of <code class="docutils literal"><span class="pre">X_train</span></code> giving an integer class label for each image &#8211; namely,
the digit between 0 and 9 depicted in the image (according to the human
annotator who drew that digit).</p>
<p>For Convolutional Neural Network example, the MNIST can be load as 4D version as follow:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> \
            <span class="n">tl</span><span class="o">.</span><span class="n">files</span><span class="o">.</span><span class="n">load_mnist_dataset</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
<p><code class="docutils literal"><span class="pre">X_train.shape</span></code> is <code class="docutils literal"><span class="pre">(50000,</span> <span class="pre">28,</span> <span class="pre">28,</span> <span class="pre">1)</span></code> which represents 50,000 images with 1 channel, 28 rows and 28 columns each.
Channel one is because it is a grey scale image, every pixel have only one value.</p>
</div>
<div class="section" id="building-the-model">
<h3>Building the model<a class="headerlink" href="#building-the-model" title="Permalink to this headline">¶</a></h3>
<p>This is where TuneLayer steps in. It allows you to define an arbitrarily
structured neural network by creating and stacking or merging layers.
Since every layer knows its immediate incoming layers, the output layer (or
output layers) of a network double as a handle to the network as a whole, so
usually this is the only thing we will pass on to the rest of the code.</p>
<p>As mentioned above, <code class="docutils literal"><span class="pre">tutorial_mnist.py</span></code> supports four types of models, and we
implement that via easily exchangeable functions of the same interface.
First, we&#8217;ll define a function that creates a Multi-Layer Perceptron (MLP) of
a fixed architecture, explaining all the steps in detail. We&#8217;ll then implement
a Denosing Autoencoder (DAE), after that we will then stack all Denoising Autoencoder and
supervised fine-tune them. Finally, we&#8217;ll show how to create a
Convolutional Neural Network (CNN).</p>
<div class="section" id="multi-layer-perceptron-mlp">
<h4>Multi-Layer Perceptron (MLP)<a class="headerlink" href="#multi-layer-perceptron-mlp" title="Permalink to this headline">¶</a></h4>
<p>The first script, <code class="docutils literal"><span class="pre">main_test_layers()</span></code>, creates an MLP of two hidden layers of
800 units each, followed by a softmax output layer of 10 units. It applies 20%
dropout to the input data and 50% dropout to the hidden layers.</p>
<p>To feed data into the network, TensofFlow placeholders need to be defined as follow.
The <code class="docutils literal"><span class="pre">None</span></code> here means the network will accept input data of arbitrary batchsize after compilation.
The <code class="docutils literal"><span class="pre">x</span></code> is used to hold the <code class="docutils literal"><span class="pre">X_train</span></code> data and <code class="docutils literal"><span class="pre">y_</span></code> is used to hold the <code class="docutils literal"><span class="pre">y_train</span></code> data.
If you know the batchsize beforehand and do not need this flexibility, you should give the batchsize
here &#8211; especially for convolutional layers, this can allow TensorFlow to apply
some optimizations.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">784</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">y_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;y_&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>The foundation of each neural network in TuneLayer is an
<code class="xref py py-class docutils literal"><span class="pre">InputLayer</span></code> instance
representing the input data that will subsequently be fed to the network. Note
that the <code class="docutils literal"><span class="pre">InputLayer</span></code> is not tied to any specific data yet.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">InputLayer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;input_layer&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Before adding the first hidden layer, we&#8217;ll apply 20% dropout to the input
data. This is realized via a <code class="xref py py-class docutils literal"><span class="pre">DropoutLayer</span></code> instance:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DropoutLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">keep</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;drop1&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that the first constructor argument is the incoming layer, the second
argument is the keeping probability for the activation value. Now we&#8217;ll proceed
with the first fully-connected hidden layer of 800 units. Note
that when stacking a <code class="xref py py-class docutils literal"><span class="pre">DenseLayer</span></code>.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">n_units</span><span class="o">=</span><span class="mi">800</span><span class="p">,</span> <span class="n">act</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;relu1&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Again, the first constructor argument means that we&#8217;re stacking <code class="docutils literal"><span class="pre">network</span></code> on
top of <code class="docutils literal"><span class="pre">network</span></code>.
<code class="docutils literal"><span class="pre">n_units</span></code> simply gives the number of units for this fully-connected layer.
<code class="docutils literal"><span class="pre">act</span></code> takes an activation function, several of which are defined
in <code class="xref py py-mod docutils literal"><span class="pre">tensorflow.nn</span></code> and <cite>tunelayer.activation</cite>. Here we&#8217;ve chosen the rectifier, so
we&#8217;ll obtain ReLUs. We&#8217;ll now add dropout of 50%, another 800-unit dense layer and 50% dropout
again:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DropoutLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">keep</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;drop2&#39;</span><span class="p">)</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">n_units</span><span class="o">=</span><span class="mi">800</span><span class="p">,</span> <span class="n">act</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;relu2&#39;</span><span class="p">)</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DropoutLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">keep</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;drop3&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Finally, we&#8217;ll add the fully-connected output layer which the <code class="docutils literal"><span class="pre">n_units</span></code> equals to
the number of classes.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span>
                              <span class="n">n_units</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                              <span class="n">act</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">identity</span><span class="p">,</span>
                              <span class="n">name</span><span class="o">=</span><span class="s1">&#39;output_layer&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>As mentioned above, each layer is linked to its incoming layer(s), so we only
need the output layer(s) to access a network in TuneLayer:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">outputs</span>
<span class="n">y_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_</span><span class="p">))</span>
</pre></div>
</div>
<p>Here, <code class="docutils literal"><span class="pre">network.outputs</span></code> is the 10 identity outputs from the network (in one hot format), <code class="docutils literal"><span class="pre">y_op</span></code> is the integer
output represents the class index. While <code class="docutils literal"><span class="pre">cost</span></code> is the cross-entropy between target and predicted labels.</p>
</div>
<div class="section" id="denoising-autoencoder-dae">
<h4>Denoising Autoencoder (DAE)<a class="headerlink" href="#denoising-autoencoder-dae" title="Permalink to this headline">¶</a></h4>
<p>Autoencoder is a unsupervised learning models which able to extract representative features,
it has become more widely used for learning generative models of data and Greedy layer-wise pre-train.
For vanilla Autoencoder see <a class="reference external" href="http://deeplearning.stanford.edu/tutorial/">Deeplearning Tutorial</a>.</p>
<p>The script <code class="docutils literal"><span class="pre">main_test_denoise_AE()</span></code> implements a Denoising Autoencoder with corrosion rate of 50%.
The Autoencoder can be defined as follow, where an Autoencoder is represented by a <code class="docutils literal"><span class="pre">DenseLayer</span></code>:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">InputLayer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;input_layer&#39;</span><span class="p">)</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DropoutLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">keep</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;denoising1&#39;</span><span class="p">)</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">n_units</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">act</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;sigmoid1&#39;</span><span class="p">)</span>
<span class="n">recon_layer1</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">ReconLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span>
                                    <span class="n">x_recon</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
                                    <span class="n">n_units</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span>
                                    <span class="n">act</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">,</span>
                                    <span class="n">name</span><span class="o">=</span><span class="s1">&#39;recon_layer1&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>To train the <code class="docutils literal"><span class="pre">DenseLayer</span></code>, simply run <code class="docutils literal"><span class="pre">ReconLayer.pretrain()</span></code>, if using denoising Autoencoder, the name of
corrosion layer (a <code class="docutils literal"><span class="pre">DropoutLayer</span></code>) need to be specified as follow. To save the feature images, set <code class="docutils literal"><span class="pre">save</span></code> to True.
There are many kinds of pre-train metrices according to different architectures and applications. For sigmoid activation,
the Autoencoder can be implemented by using KL divergence, while for rectifer, L1 regularization of activation outputs
can make the output to be sparse. So the default behaviour of <code class="docutils literal"><span class="pre">ReconLayer</span></code> only provide KLD and cross-entropy for sigmoid
activation function and L1 of activation outputs and mean-squared-error for rectifing activation function.
We recommend you to modify <code class="docutils literal"><span class="pre">ReconLayer</span></code> to achieve your own pre-train metrice.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">recon_layer1</span><span class="o">.</span><span class="n">pretrain</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span>
                      <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
                      <span class="n">X_train</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span>
                      <span class="n">X_val</span><span class="o">=</span><span class="n">X_val</span><span class="p">,</span>
                      <span class="n">denoise_name</span><span class="o">=</span><span class="s1">&#39;denoising1&#39;</span><span class="p">,</span>
                      <span class="n">n_epoch</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                      <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
                      <span class="n">print_freq</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                      <span class="n">save</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                      <span class="n">save_name</span><span class="o">=</span><span class="s1">&#39;w1pre_&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>In addition, the script <code class="docutils literal"><span class="pre">main_test_stacked_denoise_AE()</span></code> shows how to stacked multiple Autoencoder to one network and then
fine-tune.</p>
</div>
<div class="section" id="convolutional-neural-network-cnn">
<h4>Convolutional Neural Network (CNN)<a class="headerlink" href="#convolutional-neural-network-cnn" title="Permalink to this headline">¶</a></h4>
<p>Finally, the <code class="docutils literal"><span class="pre">main_test_cnn_layer()</span></code> script creates two CNN layers and
max pooling stages, a fully-connected hidden layer and a fully-connected output
layer.</p>
<p>At the begin, we add a <code class="xref py py-class docutils literal"><span class="pre">Conv2dLayer</span></code> with 32 filters of size 5x5 on top, follow by
max-pooling of factor 2 in both dimensions. And then apply a <code class="docutils literal"><span class="pre">Conv2dLayer</span></code> with
64 filters of size 5x5 again and follow by a max_pool again. After that, flatten
the 4D output to 1D vector by using <code class="docutils literal"><span class="pre">FlattenLayer</span></code>, and apply a dropout with 50%
to last hidden layer. The <code class="docutils literal"><span class="pre">?</span></code> represents arbitrary batch_size.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">InputLayer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;input_layer&#39;</span><span class="p">)</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2dLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span>
                        <span class="n">act</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span>
                        <span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">],</span>  <span class="c1"># 32 features for each 5x5 patch</span>
                        <span class="n">strides</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                        <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;SAME&#39;</span><span class="p">,</span>
                        <span class="n">name</span> <span class="o">=</span><span class="s1">&#39;cnn_layer1&#39;</span><span class="p">)</span>     <span class="c1"># output: (?, 28, 28, 32)</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">PoolLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span>
                        <span class="n">ksize</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                        <span class="n">strides</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                        <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;SAME&#39;</span><span class="p">,</span>
                        <span class="n">pool</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">max_pool</span><span class="p">,</span>
                        <span class="n">name</span> <span class="o">=</span><span class="s1">&#39;pool_layer1&#39;</span><span class="p">,)</span>   <span class="c1"># output: (?, 14, 14, 32)</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2dLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span>
                        <span class="n">act</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span>
                        <span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">],</span> <span class="c1"># 64 features for each 5x5 patch</span>
                        <span class="n">strides</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                        <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;SAME&#39;</span><span class="p">,</span>
                        <span class="n">name</span> <span class="o">=</span><span class="s1">&#39;cnn_layer2&#39;</span><span class="p">)</span>     <span class="c1"># output: (?, 14, 14, 64)</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">PoolLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span>
                        <span class="n">ksize</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                        <span class="n">strides</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                        <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;SAME&#39;</span><span class="p">,</span>
                        <span class="n">pool</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">max_pool</span><span class="p">,</span>
                        <span class="n">name</span> <span class="o">=</span><span class="s1">&#39;pool_layer2&#39;</span><span class="p">,)</span>   <span class="c1"># output: (?, 7, 7, 64)</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">FlattenLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;flatten_layer&#39;</span><span class="p">)</span>
                                                <span class="c1"># output: (?, 3136)</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DropoutLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">keep</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;drop1&#39;</span><span class="p">)</span>
                                                <span class="c1"># output: (?, 3136)</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">n_units</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">act</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;relu1&#39;</span><span class="p">)</span>
                                                <span class="c1"># output: (?, 256)</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DropoutLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">keep</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;drop2&#39;</span><span class="p">)</span>
                                                <span class="c1"># output: (?, 256)</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">n_units</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                <span class="n">act</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">identity</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;output_layer&#39;</span><span class="p">)</span>
                                                <span class="c1"># output: (?, 10)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">For experts: <code class="docutils literal"><span class="pre">Conv2dLayer</span></code> will create a convolutional layer using
<code class="docutils literal"><span class="pre">tensorflow.nn.conv2d</span></code>, TensorFlow&#8217;s default convolution.</p>
</div>
</div>
</div>
<div class="section" id="training-the-model">
<h3>Training the model<a class="headerlink" href="#training-the-model" title="Permalink to this headline">¶</a></h3>
<p>The remaining part of the <code class="docutils literal"><span class="pre">tutorial_mnist.py</span></code> script copes with setting up and running
a training loop over the MNIST dataset by using cross-entropy only.</p>
<div class="section" id="dataset-iteration">
<h4>Dataset iteration<a class="headerlink" href="#dataset-iteration" title="Permalink to this headline">¶</a></h4>
<p>An iteration function for synchronously iterating over two
numpy arrays of input data and targets, respectively, in mini-batches of a
given number of items. More iteration function can be found in <code class="docutils literal"><span class="pre">tunelayer.iterate</span></code></p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">tl</span><span class="o">.</span><span class="n">iterate</span><span class="o">.</span><span class="n">minibatches</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">batchsize</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="loss-and-update-expressions">
<h4>Loss and update expressions<a class="headerlink" href="#loss-and-update-expressions" title="Permalink to this headline">¶</a></h4>
<p>Continuing, we create a loss expression to be minimized in training:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">outputs</span>
<span class="n">y_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_</span><span class="p">))</span>
</pre></div>
</div>
<p>More cost or regularization can be applied here, take <code class="docutils literal"><span class="pre">main_test_layers()</span></code> for example,
to apply max-norm on the weight matrices, we can add the following line:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">cost</span> <span class="o">=</span> <span class="n">cost</span> <span class="o">+</span> <span class="n">tl</span><span class="o">.</span><span class="n">cost</span><span class="o">.</span><span class="n">maxnorm_regularizer</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)(</span><span class="n">network</span><span class="o">.</span><span class="n">all_params</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">+</span>
              <span class="n">tl</span><span class="o">.</span><span class="n">cost</span><span class="o">.</span><span class="n">maxnorm_regularizer</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)(</span><span class="n">network</span><span class="o">.</span><span class="n">all_params</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
</pre></div>
</div>
<p>Depending on the problem you are solving, you will need different loss functions,
see <code class="xref py py-mod docutils literal"><span class="pre">tunelayer.cost</span></code> for more.</p>
<p>Having the model and the loss function defined, we create update expressions
for training the network. TuneLayer do not provide many optimizer, we used TensorFlow&#8217;s
optimizer instead:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">train_params</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">all_params</span>
<span class="n">train_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">beta2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span>
    <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-08</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">var_list</span><span class="o">=</span><span class="n">train_params</span><span class="p">)</span>
</pre></div>
</div>
<p>For training the network, we fed data and the keeping probabilities to the <code class="docutils literal"><span class="pre">feed_dict</span></code>.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">X_train_a</span><span class="p">,</span> <span class="n">y_</span><span class="p">:</span> <span class="n">y_train_a</span><span class="p">}</span>
<span class="n">feed_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span> <span class="n">network</span><span class="o">.</span><span class="n">all_drop</span> <span class="p">)</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">train_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="n">feed_dict</span><span class="p">)</span>
</pre></div>
</div>
<p>While, for validation and testing, we use slightly different way. All
dropout, dropconnect, corrosion layers need to be disable.
<code class="docutils literal"><span class="pre">tl.utils.dict_to_one</span></code> set all <code class="docutils literal"><span class="pre">network.all_drop</span></code> to 1.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">dp_dict</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">dict_to_one</span><span class="p">(</span> <span class="n">network</span><span class="o">.</span><span class="n">all_drop</span> <span class="p">)</span>
<span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">X_test_a</span><span class="p">,</span> <span class="n">y_</span><span class="p">:</span> <span class="n">y_test_a</span><span class="p">}</span>
<span class="n">feed_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">dp_dict</span><span class="p">)</span>
<span class="n">err</span><span class="p">,</span> <span class="n">ac</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">cost</span><span class="p">,</span> <span class="n">acc</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="n">feed_dict</span><span class="p">)</span>
</pre></div>
</div>
<p>As an additional monitoring quantity, we create an expression for the
classification accuracy:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">correct_prediction</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">y_</span><span class="p">)</span>
<span class="n">acc</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct_prediction</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="what-next">
<h4>What Next?<a class="headerlink" href="#what-next" title="Permalink to this headline">¶</a></h4>
<p>We also have a more advanced image classification example in <code class="docutils literal"><span class="pre">tutorial_cifar10.py</span></code>.
Please read the code and notes, figure out how to generate more training data and what
is local response normalization. After that, try to implement
<a class="reference external" href="http://doi.org/10.3389/fpsyg.2013.00124">Residual Network</a> (Hint: you will need
to use the Layer.outputs).</p>
</div>
</div>
</div>
<div class="section" id="run-the-pong-game-example">
<h2>Run the Pong Game example<a class="headerlink" href="#run-the-pong-game-example" title="Permalink to this headline">¶</a></h2>
<p>In the second part of the tutorial, we will run the Deep Reinforcement Learning
example that is introduced by Karpathy in <a class="reference external" href="http://karpathy.github.io/2016/05/31/rl/">Deep Reinforcement Learning: Pong from Pixels</a>.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>python tutorial_atari_pong.py
</pre></div>
</div>
<p>Before running the tutorial code, you need to install <a class="reference external" href="https://gym.openai.com/docs">OpenAI gym environment</a>
which is a benchmark for Reinforcement Learning.
If everything is set up correctly, you will get an output like the following:</p>
<div class="highlight-text"><div class="highlight"><pre><span></span>[2016-07-12 09:31:59,760] Making new env: Pong-v0
  TuneLayer:Instantiate InputLayer input_layer (?, 6400)
  TuneLayer:Instantiate DenseLayer relu1: 200, &lt;function relu at 0x1119471e0&gt;
  TuneLayer:Instantiate DenseLayer output_layer: 3, &lt;function identity at 0x114bd39d8&gt;
  param 0: (6400, 200) (mean: -0.000009, median: -0.000018 std: 0.017393)
  param 1: (200,) (mean: 0.000000, median: 0.000000 std: 0.000000)
  param 2: (200, 3) (mean: 0.002239, median: 0.003122 std: 0.096611)
  param 3: (3,) (mean: 0.000000, median: 0.000000 std: 0.000000)
  num of params: 1280803
  layer 0: Tensor(&quot;Relu:0&quot;, shape=(?, 200), dtype=float32)
  layer 1: Tensor(&quot;add_1:0&quot;, shape=(?, 3), dtype=float32)
episode 0: game 0 took 0.17381s, reward: -1.000000
episode 0: game 1 took 0.12629s, reward: 1.000000  !!!!!!!!
episode 0: game 2 took 0.17082s, reward: -1.000000
episode 0: game 3 took 0.08944s, reward: -1.000000
episode 0: game 4 took 0.09446s, reward: -1.000000
episode 0: game 5 took 0.09440s, reward: -1.000000
episode 0: game 6 took 0.32798s, reward: -1.000000
episode 0: game 7 took 0.74437s, reward: -1.000000
episode 0: game 8 took 0.43013s, reward: -1.000000
episode 0: game 9 took 0.42496s, reward: -1.000000
episode 0: game 10 took 0.37128s, reward: -1.000000
episode 0: game 11 took 0.08979s, reward: -1.000000
episode 0: game 12 took 0.09138s, reward: -1.000000
episode 0: game 13 took 0.09142s, reward: -1.000000
episode 0: game 14 took 0.09639s, reward: -1.000000
episode 0: game 15 took 0.09852s, reward: -1.000000
episode 0: game 16 took 0.09984s, reward: -1.000000
episode 0: game 17 took 0.09575s, reward: -1.000000
episode 0: game 18 took 0.09416s, reward: -1.000000
episode 0: game 19 took 0.08674s, reward: -1.000000
episode 0: game 20 took 0.09628s, reward: -1.000000
resetting env. episode reward total was -20.000000. running mean: -20.000000
episode 1: game 0 took 0.09910s, reward: -1.000000
episode 1: game 1 took 0.17056s, reward: -1.000000
episode 1: game 2 took 0.09306s, reward: -1.000000
episode 1: game 3 took 0.09556s, reward: -1.000000
episode 1: game 4 took 0.12520s, reward: 1.000000  !!!!!!!!
episode 1: game 5 took 0.17348s, reward: -1.000000
episode 1: game 6 took 0.09415s, reward: -1.000000
</pre></div>
</div>
<p>This example allow computer to learn how to play Pong game from the screen inputs,
just like human behavior. After training for 15,000 episodes, the computer can
win 20% of the games. The computer win 35% of the games at 20,000 episode,
we can seen the computer learn faster and faster as it has more winning data to
train. If you run it for 30,000 episode, it start to win.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">render</span> <span class="o">=</span> <span class="bp">False</span>
<span class="n">resume</span> <span class="o">=</span> <span class="bp">False</span>
</pre></div>
</div>
<p>Setting &#8216;render&#8217; to &#8216;True&#8217;, if you want to display the game environment. When
you run the code again, you can set &#8216;resume&#8217; to &#8216;True&#8217;, the code will load the
existing model and train the model basic on it.</p>
<a class="reference internal image-reference" href="../_images/pong_game.jpeg" id="id2"><img alt="../_images/pong_game.jpeg" class="align-center" id="id2" src="../_images/pong_game.jpeg" style="width: 280.8px; height: 368.4px;" /></a>
</div>
<div class="section" id="understand-reinforcement-learning">
<h2>Understand Reinforcement learning<a class="headerlink" href="#understand-reinforcement-learning" title="Permalink to this headline">¶</a></h2>
<div class="section" id="pong-game">
<h3>Pong Game<a class="headerlink" href="#pong-game" title="Permalink to this headline">¶</a></h3>
<p>To understand Reinforcement Learning, we let computer to learn how to play
Pong game from the original screen inputs. Before we start, we highly recommend
you to go through a famous blog called <a class="reference external" href="http://karpathy.github.io/2016/05/31/rl/">Deep Reinforcement Learning: Pong from Pixels</a>
which is a minimalistic implementation of Deep Reinforcement Learning by
using python-numpy and OpenAI gym environment.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>python tutorial_atari_pong.py
</pre></div>
</div>
</div>
<div class="section" id="policy-network">
<h3>Policy Network<a class="headerlink" href="#policy-network" title="Permalink to this headline">¶</a></h3>
<p>In Deep Reinforcement Learning, the Policy Network is the same with Deep Neural
Network, it is our player (or “agent”) who output actions to tell what we should
do (move UP or DOWN); in Karpathy&#8217;s code, he only defined 2 actions, UP and DOWN
and using a single simgoid output;
In order to make our tutorial more generic, we defined 3 actions which are UP,
DOWN and STOP (do nothing) by using 3 softmax outputs.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1"># observation for training</span>
<span class="n">states_batch_pl</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="n">D</span><span class="p">])</span>

<span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">InputLayer</span><span class="p">(</span><span class="n">states_batch_pl</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;input_layer&#39;</span><span class="p">)</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">n_units</span><span class="o">=</span><span class="n">H</span><span class="p">,</span>
                                <span class="n">act</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;relu1&#39;</span><span class="p">)</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">n_units</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                        <span class="n">act</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">identity</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;output_layer&#39;</span><span class="p">)</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">outputs</span>
<span class="n">sampling_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span>
</pre></div>
</div>
<p>Then when our agent is playing Pong, it calculates the probabilities of different
actions, and then draw sample (action) from this uniform distribution. As the
actions are represented by 1, 2 and 3, but the softmax outputs should be start
from 0, we calculate the label value by minus 1.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">prob</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
    <span class="n">sampling_prob</span><span class="p">,</span>
    <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">states_batch_pl</span><span class="p">:</span> <span class="n">x</span><span class="p">}</span>
<span class="p">)</span>
<span class="c1"># action. 1: STOP  2: UP  3: DOWN</span>
<span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="n">p</span><span class="o">=</span><span class="n">prob</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>
<span class="o">...</span>
<span class="n">ys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">action</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="policy-gradient">
<h3>Policy Gradient<a class="headerlink" href="#policy-gradient" title="Permalink to this headline">¶</a></h3>
<p>The key of Deep Reinforcement Learning is how to train the Policy Network,
there are many way to do</p>
<p>Q-learning xxxxx</p>
<p>AlphaGo is using xxxx</p>
<div class="section" id="id4">
<h4>Dataset iteration<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h4>
<p>In Reinforcement Learning, we consider a final decision as an episode.
In Pong game, a episode is a few dozen games, because the games go up to score
of 21 for either player. Then the batch size is how many episode we consider
to update the model.
In the tutorial, we train a 2-layer policy network with 200 hidden layer units
using RMSProp on batches of 10 episodes.</p>
</div>
<div class="section" id="id5">
<h4>Loss and update expressions<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h4>
<p>Continuing, we create a loss expression to be minimized in training:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">actions_batch_pl</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">])</span>
<span class="n">discount_rewards_batch_pl</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">])</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">rein</span><span class="o">.</span><span class="n">cross_entropy_reward_loss</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">actions_batch_pl</span><span class="p">,</span>
                                              <span class="n">discount_rewards_batch_pl</span><span class="p">)</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
    <span class="n">train_op</span><span class="p">,</span>
    <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span>
        <span class="n">states_batch_pl</span><span class="p">:</span> <span class="n">epx</span><span class="p">,</span>
        <span class="n">actions_batch_pl</span><span class="p">:</span> <span class="n">epy</span><span class="p">,</span>
        <span class="n">discount_rewards_batch_pl</span><span class="p">:</span> <span class="n">disR</span>
    <span class="p">}</span>
<span class="p">)</span>
</pre></div>
</div>
<p>The loss in a batch is relate to all outputs of Policy Network, all actions we
made and the corresponding discounted rewards in a batch. We first compute the
loss of each action by multiplying the discounted reward and the cross-entropy
between its output and its true action. The final loss in a batch is the sum of
all loss of the actions.</p>
</div>
</div>
<div class="section" id="id6">
<h3>What Next?<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<p>The tutorial above shows how you can build your own agent, end-to-end.
While it has reasonable quality, the default parameters will not give you
the best agent model. Here are a few things you can improve.</p>
<p>First of all, instead of conventional MLP model, we can use CNNs to capture the
screen information better as <a class="reference external" href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">Playing Atari with Deep Reinforcement Learning</a>
describe.</p>
<p>Also, the default parameters of the model are not tuned. You can try changing
the learning rate, decay, or initializing the weights of your model in a
different way.</p>
<p>Finally, you can try the model on different tasks (games).</p>
</div>
</div>
<div class="section" id="run-the-word2vec-example">
<h2>Run the Word2Vec example<a class="headerlink" href="#run-the-word2vec-example" title="Permalink to this headline">¶</a></h2>
<p>In this part of the tutorial, we train a matrix for words, where each word can
be represented by a unique row vector in the matrix. In the end, similar words
will have similar vectors. Then as we plot out the words into a two-dimensional
plane, words that are similar end up clustering nearby each other</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>python tutorial_word2vec_basic.py
</pre></div>
</div>
<p>If everything is set up correctly, you will get an output in the end.</p>
<a class="reference internal image-reference" href="../_images/tsne.png" id="id7"><img alt="../_images/tsne.png" class="align-center" id="id7" src="../_images/tsne.png" style="width: 883.0px; height: 873.0px;" /></a>
</div>
<div class="section" id="understand-word-embedding">
<h2>Understand Word Embedding<a class="headerlink" href="#understand-word-embedding" title="Permalink to this headline">¶</a></h2>
<div class="section" id="word-embedding">
<h3>Word Embedding<a class="headerlink" href="#word-embedding" title="Permalink to this headline">¶</a></h3>
<p>Hao Dong highly recommend you to read Colah&#8217;s blog <a class="reference external" href="http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/">Word Representations</a> to
understand why we want to use a vector representation, and how to compute the
vectors.</p>
<p>Train an embedding matrix</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1"># train_inputs is a row vector, a input is an integer id of single word.</span>
<span class="c1"># train_labels is a column vector, a label is an integer id of single word.</span>
<span class="c1"># valid_dataset is a column vector, a valid set is an integer id of single word.</span>
<span class="n">train_inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">batch_size</span><span class="p">])</span>
<span class="n">train_labels</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">valid_examples</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

<span class="c1"># Look up embeddings for inputs.</span>
<span class="n">emb_net</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Word2vecEmbeddingInputlayer</span><span class="p">(</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">train_inputs</span><span class="p">,</span>
        <span class="n">train_labels</span> <span class="o">=</span> <span class="n">train_labels</span><span class="p">,</span>
        <span class="n">vocabulary_size</span> <span class="o">=</span> <span class="n">vocabulary_size</span><span class="p">,</span>
        <span class="n">embedding_size</span> <span class="o">=</span> <span class="n">embedding_size</span><span class="p">,</span>
        <span class="n">num_sampled</span> <span class="o">=</span> <span class="n">num_sampled</span><span class="p">,</span>
        <span class="n">nce_loss_args</span> <span class="o">=</span> <span class="p">{},</span>
        <span class="n">E_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random_uniform_initializer</span><span class="p">(</span><span class="n">minval</span><span class="o">=-</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="mf">1.0</span><span class="p">),</span>
        <span class="n">E_init_args</span> <span class="o">=</span> <span class="p">{},</span>
        <span class="n">nce_W_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal_initializer</span><span class="p">(</span>
                          <span class="n">stddev</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="mf">1.0</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">embedding_size</span><span class="p">))),</span>
        <span class="n">nce_W_init_args</span> <span class="o">=</span> <span class="p">{},</span>
        <span class="n">nce_b_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="mf">0.0</span><span class="p">),</span>
        <span class="n">nce_b_init_args</span> <span class="o">=</span> <span class="p">{},</span>
        <span class="n">name</span> <span class="o">=</span><span class="s1">&#39;word2vec_layer&#39;</span><span class="p">,</span>
    <span class="p">)</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">emb_net</span><span class="o">.</span><span class="n">nce_cost</span>
</pre></div>
</div>
<div class="section" id="id8">
<h4>Dataset iteration<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h4>
</div>
<div class="section" id="id9">
<h4>Loss and update expressions<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h4>
</div>
<div class="section" id="load-an-embedding-matrix">
<h4>Load an Embedding matrix<a class="headerlink" href="#load-an-embedding-matrix" title="Permalink to this headline">¶</a></h4>
<p>In the end of training the embedding matrix,</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>python tutorial_generate_text.py
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">load_params</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">files</span><span class="o">.</span><span class="n">load_npz</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">model_file_name</span><span class="o">+</span><span class="s1">&#39;.npz&#39;</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">batch_size</span><span class="p">])</span>
<span class="n">y_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="n">emb_net</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">EmbeddingInputlayer</span><span class="p">(</span>
                <span class="n">inputs</span> <span class="o">=</span> <span class="n">x</span><span class="p">,</span>
                <span class="n">vocabulary_size</span> <span class="o">=</span> <span class="n">vocabulary_size</span><span class="p">,</span>
                <span class="n">embedding_size</span> <span class="o">=</span> <span class="n">embedding_size</span><span class="p">,</span>
                <span class="n">name</span> <span class="o">=</span><span class="s1">&#39;embedding_layer&#39;</span><span class="p">)</span>

<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">initialize_all_variables</span><span class="p">())</span>

<span class="n">tl</span><span class="o">.</span><span class="n">files</span><span class="o">.</span><span class="n">assign_params</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="p">[</span><span class="n">load_params</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="n">emb_net</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="run-the-ptb-example">
<h2>Run the PTB example<a class="headerlink" href="#run-the-ptb-example" title="Permalink to this headline">¶</a></h2>
<p>Penn TreeBank (PTB) dataset is used in many LANGUAGE MODELING papers,
including &#8220;Empirical Evaluation and Combination of Advanced Language
Modeling Techniques&#8221;, &#8220;Recurrent Neural Network Regularization&#8221;.
It consists of 929k training words, 73k validation words, and 82k test
words. It has 10k words in its vocabulary.</p>
<p>The PTB example is trying to show how to train a recurrent neural network on a
challenging task of language modeling.</p>
<p>Given a sentence &#8220;I am from Imperial College London&#8221;, the model can learn to
predict &#8220;Imperial College London&#8221; from &#8220;from Imperial College&#8221;. In other
word, it predict next words in a text given a history of previous words.
In this case, <code class="docutils literal"><span class="pre">num_steps</span> <span class="pre">(sequence</span> <span class="pre">length)</span></code> is 3.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>python tutorial_ptb_lstm.py
</pre></div>
</div>
<p>The script provides three settings (small, medium, large), larger model has
better performance, you can choice different setting in:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_string</span><span class="p">(</span>
    <span class="s2">&quot;model&quot;</span><span class="p">,</span> <span class="s2">&quot;small&quot;</span><span class="p">,</span>
    <span class="s2">&quot;A type of model. Possible options are: small, medium, large.&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>If you choice small setting, you can see:</p>
<div class="highlight-text"><div class="highlight"><pre><span></span>Epoch: 1 Learning rate: 1.000
0.004 perplexity: 5220.213 speed: 7635 wps
0.104 perplexity: 828.871 speed: 8469 wps
0.204 perplexity: 614.071 speed: 8839 wps
0.304 perplexity: 495.485 speed: 8889 wps
0.404 perplexity: 427.381 speed: 8940 wps
0.504 perplexity: 383.063 speed: 8920 wps
0.604 perplexity: 345.135 speed: 8920 wps
0.703 perplexity: 319.263 speed: 8949 wps
0.803 perplexity: 298.774 speed: 8975 wps
0.903 perplexity: 279.817 speed: 8986 wps
Epoch: 1 Train Perplexity: 265.558
Epoch: 1 Valid Perplexity: 178.436
...
Epoch: 13 Learning rate: 0.004
0.004 perplexity: 56.122 speed: 8594 wps
0.104 perplexity: 40.793 speed: 9186 wps
0.204 perplexity: 44.527 speed: 9117 wps
0.304 perplexity: 42.668 speed: 9214 wps
0.404 perplexity: 41.943 speed: 9269 wps
0.504 perplexity: 41.286 speed: 9271 wps
0.604 perplexity: 39.989 speed: 9244 wps
0.703 perplexity: 39.403 speed: 9236 wps
0.803 perplexity: 38.742 speed: 9229 wps
0.903 perplexity: 37.430 speed: 9240 wps
Epoch: 13 Train Perplexity: 36.643
Epoch: 13 Valid Perplexity: 121.475
Test Perplexity: 116.716
</pre></div>
</div>
<p>The PTB example proves RNN is able to modeling language, but this example
did not do something practical. However, you should read through this example
and <code class="docutils literal"><span class="pre">Understand</span> <span class="pre">LSTM</span></code> in order to understand the basic of RNN.
After that, you learn how to generate text, how to achieve language translation
and how to build a questions answering system by using RNN.</p>
</div>
<div class="section" id="understand-lstm">
<h2>Understand LSTM<a class="headerlink" href="#understand-lstm" title="Permalink to this headline">¶</a></h2>
<div class="section" id="recurrent-neural-network">
<h3>Recurrent Neural Network<a class="headerlink" href="#recurrent-neural-network" title="Permalink to this headline">¶</a></h3>
<p>Hao Dong personally think Andrey Karpathy&#8217;s blog is the best material to
<a class="reference external" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">Understand Recurrent Neural Network</a> , after reading that, Colah&#8217;s blog can
help you to <a class="reference external" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understand LSTM Network</a> which can solve The Problem of Long-Term
Dependencies. We do not describe more about RNN, please read through these blogs
before you go on.</p>
<img alt="../_images/karpathy_rnn.jpeg" id="id10" src="../_images/karpathy_rnn.jpeg" />
<p>Image by Andrey Karpathy</p>
</div>
<div class="section" id="synced-sequence-input-and-output">
<h3>Synced sequence input and output<a class="headerlink" href="#synced-sequence-input-and-output" title="Permalink to this headline">¶</a></h3>
<p>The model in PTB example is a typically type of synced sequence input and output,
which was described by Karpathy as
&#8220;(5) Synced sequence input and output (e.g. video classification where we wish
to label each frame of the video). Notice that in every case are no pre-specified
constraints on the lengths sequences because the recurrent transformation (green)
is fixed and can be applied as many times as we like.&#8221;</p>
<p>The model is built as follow. Firstly, transfer the words into word vectors by
looking up an embedding matrix. In this tutorial, no pre-training on embedding
matrix. Secondly, we stacked two LSTMs together use dropout among the embedding
layer, LSTM layers and output layer for regularization. The model provides
a sequence of softmax outputs during training.</p>
<p>The first LSTM layer outputs [batch_size, num_steps, hidden_size] for stacking
another LSTM after it. The second LSTM layer outputs [batch_size*num_steps, hidden_size]
for stacking DenseLayer after it, then compute the softmax outputs of each example,
i.e. n_examples = batch_size*num_steps.</p>
<p>To understand the PTB tutorial, you can also read <a class="reference external" href="https://www.tensorflow.org/versions/r0.9/tutorials/recurrent/index.html#recurrent-neural-networks">TensorFlow PTB tutorial</a>.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">EmbeddingInputlayer</span><span class="p">(</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="n">x</span><span class="p">,</span>
            <span class="n">vocabulary_size</span> <span class="o">=</span> <span class="n">vocab_size</span><span class="p">,</span>
            <span class="n">embedding_size</span> <span class="o">=</span> <span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">E_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random_uniform_initializer</span><span class="p">(</span><span class="o">-</span><span class="n">init_scale</span><span class="p">,</span> <span class="n">init_scale</span><span class="p">),</span>
            <span class="n">name</span> <span class="o">=</span><span class="s1">&#39;embedding_layer&#39;</span><span class="p">)</span>
<span class="k">if</span> <span class="n">is_training</span><span class="p">:</span>
    <span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DropoutLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">keep</span><span class="o">=</span><span class="n">keep_prob</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;drop1&#39;</span><span class="p">)</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">RNNLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span>
            <span class="n">cell_fn</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">rnn_cell</span><span class="o">.</span><span class="n">BasicLSTMCell</span><span class="p">,</span>
            <span class="n">cell_init_args</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;forget_bias&#39;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">},</span>
            <span class="n">n_hidden</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">random_uniform_initializer</span><span class="p">(</span><span class="o">-</span><span class="n">init_scale</span><span class="p">,</span> <span class="n">init_scale</span><span class="p">),</span>
            <span class="n">n_steps</span><span class="o">=</span><span class="n">num_steps</span><span class="p">,</span>
            <span class="n">return_last</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s1">&#39;basic_lstm_layer1&#39;</span><span class="p">)</span>
<span class="n">lstm1</span> <span class="o">=</span> <span class="n">network</span>
<span class="k">if</span> <span class="n">is_training</span><span class="p">:</span>
    <span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DropoutLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">keep</span><span class="o">=</span><span class="n">keep_prob</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;drop2&#39;</span><span class="p">)</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">RNNLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span>
            <span class="n">cell_fn</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">rnn_cell</span><span class="o">.</span><span class="n">BasicLSTMCell</span><span class="p">,</span>
            <span class="n">cell_init_args</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;forget_bias&#39;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">},</span>
            <span class="n">n_hidden</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">random_uniform_initializer</span><span class="p">(</span><span class="o">-</span><span class="n">init_scale</span><span class="p">,</span> <span class="n">init_scale</span><span class="p">),</span>
            <span class="n">n_steps</span><span class="o">=</span><span class="n">num_steps</span><span class="p">,</span>
            <span class="n">return_last</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
            <span class="n">return_seq_2d</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s1">&#39;basic_lstm_layer2&#39;</span><span class="p">)</span>
<span class="n">lstm2</span> <span class="o">=</span> <span class="n">network</span>
<span class="k">if</span> <span class="n">is_training</span><span class="p">:</span>
    <span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DropoutLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">keep</span><span class="o">=</span><span class="n">keep_prob</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;drop3&#39;</span><span class="p">)</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span>
            <span class="n">n_units</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
            <span class="n">W_init</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">random_uniform_initializer</span><span class="p">(</span><span class="o">-</span><span class="n">init_scale</span><span class="p">,</span> <span class="n">init_scale</span><span class="p">),</span>
            <span class="n">b_init</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">random_uniform_initializer</span><span class="p">(</span><span class="o">-</span><span class="n">init_scale</span><span class="p">,</span> <span class="n">init_scale</span><span class="p">),</span>
            <span class="n">act</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">identity</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;output_layer&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="id11">
<h4>Dataset iteration<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h4>
<p>The batch_size can be seem as how many concurrent computations.
As the following example shows, the first batch learn the sequence information by using 0 to 9.
The second batch learn the sequence information by using 10 to 19.
So it ignores the information from 9 to 10 !n
If only if we set the batch_size = 1, it will consider all information from 0 to 20.</p>
<p>The meaning of batch_size here is not the same with the MNIST example. In MNIST example,
batch_size reflects how many examples we consider in each iteration, while in
PTB example, batch_size is how many concurrent processes (segments)
for speed up computation.</p>
<p>Some Information will be ignored if batch_size &gt; 1, however, if your dataset
is &#8220;long&#8221; enough (a text corpus usually has billions words), the ignored
information would not effect the final result.</p>
<p>In PTB tutorial, we setted batch_size = 20, so we cut the dataset into 20 segments.
At the begining of each epoch, we initialize (reset) the 20 RNN states for 20
segments, then go through 20 segments separately.</p>
<p>The training data will be generated as follow:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">train_data</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">)]</span>
<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">tl</span><span class="o">.</span><span class="n">iterate</span><span class="o">.</span><span class="n">ptb_iterator</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text"><div class="highlight"><pre><span></span>... [[ 0  1  2] &lt;---x                       1st subset/ iteration
...  [10 11 12]]
... [[ 1  2  3] &lt;---y
...  [11 12 13]]
...
... [[ 3  4  5]  &lt;--- 1st batch input       2nd subset/ iteration
...  [13 14 15]] &lt;--- 2nd batch input
... [[ 4  5  6]  &lt;--- 1st batch target
...  [14 15 16]] &lt;--- 2nd batch target
...
... [[ 6  7  8]                             3rd subset/ iteration
...  [16 17 18]]
... [[ 7  8  9]
...  [17 18 19]]
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This example can also be considered as pre-training of the word embedding matrix.</p>
</div>
</div>
<div class="section" id="id12">
<h4>Loss and update expressions<a class="headerlink" href="#id12" title="Permalink to this headline">¶</a></h4>
<p>The cost function is the averaged cost of each mini-batch:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">):</span>
    <span class="c1"># Returns the cost function of Cross-entropy of two sequences, implement</span>
    <span class="c1"># softmax internally.</span>
    <span class="c1"># outputs : 2D tensor [batch_size*num_steps, n_units of output layer]</span>
    <span class="c1"># targets : 2D tensor [batch_size, num_steps], need to be reshaped.</span>
    <span class="c1"># n_examples = batch_size * num_steps</span>
    <span class="c1"># so</span>
    <span class="c1"># cost is the averaged cost of each mini-batch (concurrent process).</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">seq2seq</span><span class="o">.</span><span class="n">sequence_loss_by_example</span><span class="p">(</span>
        <span class="p">[</span><span class="n">outputs</span><span class="p">],</span>
        <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])],</span>
        <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_steps</span><span class="p">])])</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span> <span class="o">/</span> <span class="n">batch_size</span>
    <span class="k">return</span> <span class="n">cost</span>

<span class="c1"># Cost for Training</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">)</span>
</pre></div>
</div>
<p>For updating, this example decreases the initial learning rate after several
epoachs (defined by <code class="docutils literal"><span class="pre">max_epoch</span></code>), by multipling a <code class="docutils literal"><span class="pre">lr_decay</span></code>. In addition,
truncated backpropagation clips values of gradients by the ratio of the sum of
their norms, so as to make the learning process tractable.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1"># Truncated Backpropagation for training</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">):</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">tvars</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">()</span>
<span class="n">grads</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_global_norm</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">tvars</span><span class="p">),</span>
                                  <span class="n">max_grad_norm</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>
<span class="n">train_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">tvars</span><span class="p">))</span>
</pre></div>
</div>
<p>Then at the beginning of each epoch, we assign a new learning rate:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">new_lr_decay</span> <span class="o">=</span> <span class="n">lr_decay</span> <span class="o">**</span> <span class="nb">max</span><span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="n">max_epoch</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">new_lr_decay</span><span class="p">))</span>
</pre></div>
</div>
<p>At the begining of each epoch, all states of LSTMs need to be resetted (initialized),
then after each iteration, the new final states need to be assigned as the initial
states of next iteration:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">state1</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">initialize_rnn_state</span><span class="p">(</span><span class="n">lstm1</span><span class="o">.</span><span class="n">initial_state</span><span class="p">)</span>
<span class="n">state2</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">initialize_rnn_state</span><span class="p">(</span><span class="n">lstm2</span><span class="o">.</span><span class="n">initial_state</span><span class="p">)</span>
<span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tl</span><span class="o">.</span><span class="n">iterate</span><span class="o">.</span><span class="n">ptb_iterator</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span>
                                            <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">)):</span>
    <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">input_data</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">targets</span><span class="p">:</span> <span class="n">y</span><span class="p">,</span>
                <span class="n">lstm1</span><span class="o">.</span><span class="n">initial_state</span><span class="p">:</span> <span class="n">state1</span><span class="p">,</span>
                <span class="n">lstm2</span><span class="o">.</span><span class="n">initial_state</span><span class="p">:</span> <span class="n">state2</span><span class="p">,</span>
                <span class="p">}</span>
    <span class="c1"># For training, enable dropout</span>
    <span class="n">feed_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span> <span class="n">network</span><span class="o">.</span><span class="n">all_drop</span> <span class="p">)</span>
    <span class="n">_cost</span><span class="p">,</span> <span class="n">state1</span><span class="p">,</span> <span class="n">state2</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">cost</span><span class="p">,</span>
                                    <span class="n">lstm1</span><span class="o">.</span><span class="n">final_state</span><span class="p">,</span>
                                    <span class="n">lstm2</span><span class="o">.</span><span class="n">final_state</span><span class="p">,</span>
                                    <span class="n">train_op</span><span class="p">],</span>
                                    <span class="n">feed_dict</span><span class="o">=</span><span class="n">feed_dict</span>
                                    <span class="p">)</span>
    <span class="n">costs</span> <span class="o">+=</span> <span class="n">_cost</span><span class="p">;</span> <span class="n">iters</span> <span class="o">+=</span> <span class="n">num_steps</span>
</pre></div>
</div>
</div>
<div class="section" id="predicting">
<h4>Predicting<a class="headerlink" href="#predicting" title="Permalink to this headline">¶</a></h4>
<p>After training the model, we no long consider the number of steps (sequence length),
i.e. <code class="docutils literal"><span class="pre">batch_size,</span> <span class="pre">num_steps</span></code> are <code class="docutils literal"><span class="pre">1</span></code>. Then we can predict the next word step
by step, instead of predict a sequence of words from a sequence of words.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">input_data_test</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">targets_test</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="o">...</span>
<span class="n">network_test</span><span class="p">,</span> <span class="n">lstm1_test</span><span class="p">,</span> <span class="n">lstm2_test</span> <span class="o">=</span> <span class="n">inference</span><span class="p">(</span><span class="n">input_data_test</span><span class="p">,</span>
                      <span class="n">is_training</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">reuse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="o">...</span>
<span class="n">cost_test</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">network_test</span><span class="o">.</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets_test</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="o">...</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Evaluation&quot;</span><span class="p">)</span>
<span class="c1"># Testing</span>
<span class="c1"># go through the test set step by step, it will take a while.</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">costs</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span> <span class="n">iters</span> <span class="o">=</span> <span class="mi">0</span>
<span class="c1"># reset all states at the begining</span>
<span class="n">state1</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">initialize_rnn_state</span><span class="p">(</span><span class="n">lstm1_test</span><span class="o">.</span><span class="n">initial_state</span><span class="p">)</span>
<span class="n">state2</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">initialize_rnn_state</span><span class="p">(</span><span class="n">lstm2_test</span><span class="o">.</span><span class="n">initial_state</span><span class="p">)</span>
<span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tl</span><span class="o">.</span><span class="n">iterate</span><span class="o">.</span><span class="n">ptb_iterator</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span>
                                        <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">)):</span>
    <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">input_data_test</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">targets_test</span><span class="p">:</span> <span class="n">y</span><span class="p">,</span>
                <span class="n">lstm1_test</span><span class="o">.</span><span class="n">initial_state</span><span class="p">:</span> <span class="n">state1</span><span class="p">,</span>
                <span class="n">lstm2_test</span><span class="o">.</span><span class="n">initial_state</span><span class="p">:</span> <span class="n">state2</span><span class="p">,</span>
                <span class="p">}</span>
    <span class="n">_cost</span><span class="p">,</span> <span class="n">state1</span><span class="p">,</span> <span class="n">state2</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">cost_test</span><span class="p">,</span>
                                    <span class="n">lstm1_test</span><span class="o">.</span><span class="n">final_state</span><span class="p">,</span>
                                    <span class="n">lstm2_test</span><span class="o">.</span><span class="n">final_state</span><span class="p">],</span>
                                    <span class="n">feed_dict</span><span class="o">=</span><span class="n">feed_dict</span>
                                    <span class="p">)</span>
    <span class="n">costs</span> <span class="o">+=</span> <span class="n">_cost</span><span class="p">;</span> <span class="n">iters</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="n">test_perplexity</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">costs</span> <span class="o">/</span> <span class="n">iters</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Test Perplexity: </span><span class="si">%.3f</span><span class="s2"> took </span><span class="si">%.2f</span><span class="s2">s&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">test_perplexity</span><span class="p">,</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id13">
<h3>What Next?<a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h3>
<p>Now, you understand Synced sequence input and output. Let think about
Many to one (Sequence input and one output), we can also use &#8220;I am from Imperial&#8221;
to predict the next word &#8220;College&#8221; right? Please try your best to build a text
generator, which give some seed words to generate context, some people even used
Many to one model to automatically generate papers !</p>
<p>Karpathy&#8217;s blog :
&#8220;(3) Sequence input (e.g. sentiment analysis where a given sentence is
classified as expressing positive or negative sentiment). &#8220;</p>
</div>
</div>
<div class="section" id="run-the-translation-example">
<h2>Run the Translation example<a class="headerlink" href="#run-the-translation-example" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">tutorial_translate</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
<p>This script is going to training a neural network to translate English to French.
If everything is correct, you will see.</p>
<ul class="simple">
<li>Download WMT English-to-French translation data, includes training and testing data.</li>
<li>Create vocabulary files for English and French from training data.</li>
<li>Create the tokenized training and testing data from original training and
testing data.</li>
</ul>
<div class="highlight-bash"><div class="highlight"><pre><span></span>Load or Download WMT English-to-French translation &gt; wmt
wmt/giga-fren.release2
wmt/newstest2013
wmt/vocab40000.fr
wmt/vocab40000.en
Creating vocabulary wmt/vocab40000.fr from data wmt/giga-fren.release2.fr
  processing line 100000
  processing line 200000
  processing line 300000
  processing line 400000
  processing line 500000
  processing line 600000
  processing line 700000
  processing line 800000
  processing line 900000
  processing line 1000000
  processing line 1100000
  processing line 1200000
  ...
  processing line 22500000
Creating vocabulary wmt/vocab40000.en from data wmt/giga-fren.release2.en
  processing line 100000
  ...
  processing line 22500000

...
</pre></div>
</div>
<p>Firstly, we download English-to-French translation data from the WMT&#8216;15
Website. The training and testing data as follow. The training data is used to
train the model, the testing data is used to XXXX.</p>
<div class="highlight-text"><div class="highlight"><pre><span></span>wmt/training-giga-fren.tar  &lt;-- Training data for English-to-French (2.6GB)
                                giga-fren.release2.* are extracted from it.
wmt/dev-v2.tgz              &lt;-- Testing data for different language (21.4MB)
                                newstest2013.* are extracted from it.

wmt/giga-fren.release2.fr   &lt;-- Training data of French   (4.57GB)
wmt/giga-fren.release2.en   &lt;-- Training data of English  (3.79GB)

wmt/newstest2013.fr         &lt;-- Testing data of French    (393KB)
wmt/newstest2013.en         &lt;-- Testing data of English   (333KB)
</pre></div>
</div>
<p>As <code class="docutils literal"><span class="pre">giga-fren.release2.*</span></code> are the training data, <code class="docutils literal"><span class="pre">giga-fren.release2.fr</span></code> look as follow.</p>
<div class="highlight-text"><div class="highlight"><pre><span></span>Il a transformé notre vie | Il a transformé la société | Son fonctionnement | La technologie, moteur du changement Accueil | Concepts | Enseignants | Recherche | Aperçu | Collaborateurs | Web HHCC | Ressources | Commentaires Musée virtuel du Canada
Plan du site
Rétroaction
Crédits
English
Qu’est-ce que la lumière?
La découverte du spectre de la lumière blanche Des codes dans la lumière Le spectre électromagnétique Les spectres d’émission Les spectres d’absorption Les années-lumière La pollution lumineuse
Le ciel des premiers habitants La vision contemporaine de l&#39;Univers L’astronomie pour tous
Bande dessinée
Liens
Glossaire
Observatoires
...
</pre></div>
</div>
<p>While <code class="docutils literal"><span class="pre">giga-fren.release2.en</span></code> look as follow, we can see words or sentences
are separated by <code class="docutils literal"><span class="pre">|</span></code> or <code class="docutils literal"><span class="pre">\n</span></code>.</p>
<div class="highlight-text"><div class="highlight"><pre><span></span>Changing Lives | Changing Society | How It Works | Technology Drives Change Home | Concepts | Teachers | Search | Overview | Credits | HHCC Web | Reference | Feedback Virtual Museum of Canada Home Page
Site map
Feedback
Credits
Français
What is light ?
The white light spectrum Codes in the light The electromagnetic spectrum Emission spectra Absorption spectra Light-years Light pollution
The sky of the first inhabitants A contemporary vison of the Universe Astronomy for everyone
Cartoon
Links
Glossary
Observatories
</pre></div>
</div>
<p>The testing data <code class="docutils literal"><span class="pre">newstest2013.en</span></code> and <code class="docutils literal"><span class="pre">newstest2013.fr</span></code> look as follow.</p>
<div class="highlight-text"><div class="highlight"><pre><span></span>newstest2013.en :
A Republican strategy to counter the re-election of Obama
Republican leaders justified their policy by the need to combat electoral fraud.
However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.

newstest2013.fr :
Une stratégie républicaine pour contrer la réélection d&#39;Obama
Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
</pre></div>
</div>
<p>After downloading the dataset, it start to create vocabulary files,
<code class="docutils literal"><span class="pre">vocab40000.fr</span></code> and <code class="docutils literal"><span class="pre">vocab40000.en</span></code> from the training data <code class="docutils literal"><span class="pre">giga-fren.release2.fr</span></code>
and <code class="docutils literal"><span class="pre">giga-fren.release2.en</span></code>, usually it will take a while. The number <code class="docutils literal"><span class="pre">40000</span></code>
reflects the vocabulary size.</p>
<p>The <code class="docutils literal"><span class="pre">vocab40000.fr</span></code> (381KB) stores one-item-per-line as follow.</p>
<div class="highlight-text"><div class="highlight"><pre><span></span>_PAD
_GO
_EOS
_UNK
de
,
.
&#39;
la
et
des
les
à
le
du
l
en
)
d
0
(
00
pour
dans
un
que
une
sur
au
0000
a
par
</pre></div>
</div>
<p>The <code class="docutils literal"><span class="pre">vocab40000.en</span></code> (344KB) stores one-item-per-line as follow.</p>
<div class="highlight-text"><div class="highlight"><pre><span></span>_PAD
_GO
_EOS
_UNK
the
.
,
of
and
to
in
a
)
(
0
for
00
that
is
on
The
0000
be
by
with
or
:
as
&quot;
000
are
;
</pre></div>
</div>
<p>And then, we start to create the tokenized training and testing data for both
English and French. It will take a while as well.</p>
<div class="highlight-text"><div class="highlight"><pre><span></span>Tokenizing data in wmt/giga-fren.release2.fr  &lt;-- Training data of French
  tokenizing line 100000
  tokenizing line 200000
  tokenizing line 300000
  tokenizing line 400000
  ...
  tokenizing line 22500000
Tokenizing data in wmt/giga-fren.release2.en  &lt;-- Training data of English
  tokenizing line 100000
  tokenizing line 200000
  tokenizing line 300000
  tokenizing line 400000
  ...
  tokenizing line 22500000
Tokenizing data in wmt/newstest2013.fr        &lt;-- Testing data of French
Tokenizing data in wmt/newstest2013.en        &lt;-- Testing data of English
</pre></div>
</div>
<p>In the end, all files we have as follow.</p>
<div class="highlight-text"><div class="highlight"><pre><span></span>wmt/training-giga-fren.tar  &lt;-- Compressed Training data for English-to-French (2.6GB)
                                giga-fren.release2.* are extracted from it.
wmt/dev-v2.tgz              &lt;-- Compressed Testing data for different language (21.4MB)
                                newstest2013.* are extracted from it.

wmt/giga-fren.release2.fr   &lt;-- Training data of French   (4.57GB)
wmt/giga-fren.release2.en   &lt;-- Training data of English  (3.79GB)

wmt/newstest2013.fr         &lt;-- Testing data of French    (393KB)
wmt/newstest2013.en         &lt;-- Testing data of English   (333KB)

wmt/vocab40000.fr           &lt;-- Vocabulary of French      (381KB)
wmt/vocab40000.en           &lt;-- Vocabulary of English     (344KB)

wmt/giga-fren.release2.ids40000.fr   &lt;-- Tokenized Training data of French (2.81GB)
wmt/giga-fren.release2.ids40000.en   &lt;-- Tokenized Training data of English (2.38GB)

wmt/newstest2013.ids40000.fr         &lt;-- Tokenized Testing data of French (268KB)
wmt/newstest2013.ids40000.en         &lt;-- Tokenized Testing data of English (232KB)
</pre></div>
</div>
<p>Now, read all tokenized data into buckets and compute their sizes.</p>
<div class="highlight-text"><div class="highlight"><pre><span></span>Reading development (testing) data into buckets
dev data: (5, 10) [[13388, 4, 949], [23113, 8, 910, 2]]
en word_ids: [13388, 4, 949]
en context: [b&#39;Preventing&#39;, b&#39;the&#39;, b&#39;disease&#39;]
fr word_ids: [23113, 8, 910, 2]
fr context: [b&#39;Pr\xc3\xa9venir&#39;, b&#39;la&#39;, b&#39;maladie&#39;, b&#39;_EOS&#39;]

Reading training data into buckets (limit: 0).
  reading data line 100000
  reading data line 200000
  reading data line 300000
  reading data line 400000
  reading data line 500000
  reading data line 600000
  reading data line 700000
  reading data line 800000
  ...
  reading data line 22400000
  reading data line 22500000
train_bucket_sizes: [239121, 1344322, 5239557, 10445326]
train_total_size: 17268326.0
train_buckets_scale: [0.013847375825543252, 0.09169638099257565, 0.3951164693091849, 1.0]
train data: (5, 10) [[1368, 3344], [1089, 14, 261, 2]]
en word_ids: [1368, 3344]
en context: [b&#39;Site&#39;, b&#39;map&#39;]
fr word_ids: [1089, 14, 261, 2]
fr context: [b&#39;Plan&#39;, b&#39;du&#39;, b&#39;site&#39;, b&#39;_EOS&#39;]
</pre></div>
</div>
<p>Start training by using the tokenized bucket data, the training process can
only be terminated by stop the program.
When <code class="docutils literal"><span class="pre">steps_per_checkpoint</span> <span class="pre">=</span> <span class="pre">10</span></code> you will see.</p>
<div class="highlight-text"><div class="highlight"><pre><span></span>global step 10 learning rate 0.5000 step-time 22.26 perplexity 12761.50
  eval: bucket 0 perplexity 5887.75
  eval: bucket 1 perplexity 3891.96
  eval: bucket 2 perplexity 3748.77
  eval: bucket 3 perplexity 4940.10
global step 20 learning rate 0.5000 step-time 20.38 perplexity 28761.36
  eval: bucket 0 perplexity 10137.01
  eval: bucket 1 perplexity 12809.90
  eval: bucket 2 perplexity 15758.65
  eval: bucket 3 perplexity 26760.93
global step 30 learning rate 0.5000 step-time 20.64 perplexity 6372.95
  eval: bucket 0 perplexity 1789.80
  eval: bucket 1 perplexity 1690.00
  eval: bucket 2 perplexity 2190.18
  eval: bucket 3 perplexity 3808.12
global step 40 learning rate 0.5000 step-time 16.10 perplexity 3418.93
  eval: bucket 0 perplexity 4778.76
  eval: bucket 1 perplexity 3698.90
  eval: bucket 2 perplexity 3902.37
  eval: bucket 3 perplexity 22612.44
global step 50 learning rate 0.5000 step-time 14.84 perplexity 1811.02
  eval: bucket 0 perplexity 644.72
  eval: bucket 1 perplexity 759.16
  eval: bucket 2 perplexity 984.18
  eval: bucket 3 perplexity 1585.68
global step 60 learning rate 0.5000 step-time 19.76 perplexity 1580.55
  eval: bucket 0 perplexity 1724.84
  eval: bucket 1 perplexity 2292.24
  eval: bucket 2 perplexity 2698.52
  eval: bucket 3 perplexity 3189.30
global step 70 learning rate 0.5000 step-time 17.16 perplexity 1250.57
  eval: bucket 0 perplexity 298.55
  eval: bucket 1 perplexity 502.04
  eval: bucket 2 perplexity 645.44
  eval: bucket 3 perplexity 604.29
global step 80 learning rate 0.5000 step-time 18.50 perplexity 793.90
  eval: bucket 0 perplexity 2056.23
  eval: bucket 1 perplexity 1344.26
  eval: bucket 2 perplexity 767.82
  eval: bucket 3 perplexity 649.38
global step 90 learning rate 0.5000 step-time 12.61 perplexity 541.57
  eval: bucket 0 perplexity 180.86
  eval: bucket 1 perplexity 350.99
  eval: bucket 2 perplexity 326.85
  eval: bucket 3 perplexity 383.22
global step 100 learning rate 0.5000 step-time 18.42 perplexity 471.12
  eval: bucket 0 perplexity 216.63
  eval: bucket 1 perplexity 348.96
  eval: bucket 2 perplexity 318.20
  eval: bucket 3 perplexity 389.92
global step 110 learning rate 0.5000 step-time 18.39 perplexity 474.89
  eval: bucket 0 perplexity 8049.85
  eval: bucket 1 perplexity 1677.24
  eval: bucket 2 perplexity 936.98
  eval: bucket 3 perplexity 657.46
global step 120 learning rate 0.5000 step-time 18.81 perplexity 832.11
  eval: bucket 0 perplexity 189.22
  eval: bucket 1 perplexity 360.69
  eval: bucket 2 perplexity 410.57
  eval: bucket 3 perplexity 456.40
global step 130 learning rate 0.5000 step-time 20.34 perplexity 452.27
  eval: bucket 0 perplexity 196.93
  eval: bucket 1 perplexity 655.18
  eval: bucket 2 perplexity 860.44
  eval: bucket 3 perplexity 1062.36
global step 140 learning rate 0.5000 step-time 21.05 perplexity 847.11
  eval: bucket 0 perplexity 391.88
  eval: bucket 1 perplexity 339.09
  eval: bucket 2 perplexity 320.08
  eval: bucket 3 perplexity 376.44
global step 150 learning rate 0.4950 step-time 15.53 perplexity 590.03
  eval: bucket 0 perplexity 269.16
  eval: bucket 1 perplexity 286.51
  eval: bucket 2 perplexity 391.78
  eval: bucket 3 perplexity 485.23
global step 160 learning rate 0.4950 step-time 19.36 perplexity 400.80
  eval: bucket 0 perplexity 137.00
  eval: bucket 1 perplexity 198.85
  eval: bucket 2 perplexity 276.58
  eval: bucket 3 perplexity 357.78
global step 170 learning rate 0.4950 step-time 17.50 perplexity 541.79
  eval: bucket 0 perplexity 1051.29
  eval: bucket 1 perplexity 626.64
  eval: bucket 2 perplexity 496.32
  eval: bucket 3 perplexity 458.85
global step 180 learning rate 0.4950 step-time 16.69 perplexity 400.65
  eval: bucket 0 perplexity 178.12
  eval: bucket 1 perplexity 299.86
  eval: bucket 2 perplexity 294.84
  eval: bucket 3 perplexity 296.46
global step 190 learning rate 0.4950 step-time 19.93 perplexity 886.73
  eval: bucket 0 perplexity 860.60
  eval: bucket 1 perplexity 910.16
  eval: bucket 2 perplexity 909.24
  eval: bucket 3 perplexity 786.04
global step 200 learning rate 0.4901 step-time 18.75 perplexity 449.64
  eval: bucket 0 perplexity 152.13
  eval: bucket 1 perplexity 234.41
  eval: bucket 2 perplexity 249.66
  eval: bucket 3 perplexity 285.95
...
global step 980 learning rate 0.4215 step-time 18.31 perplexity 208.74
  eval: bucket 0 perplexity 78.45
  eval: bucket 1 perplexity 108.40
  eval: bucket 2 perplexity 137.83
  eval: bucket 3 perplexity 173.53
global step 990 learning rate 0.4173 step-time 17.31 perplexity 175.05
  eval: bucket 0 perplexity 78.37
  eval: bucket 1 perplexity 119.72
  eval: bucket 2 perplexity 169.11
  eval: bucket 3 perplexity 202.89
global step 1000 learning rate 0.4173 step-time 15.85 perplexity 174.33
  eval: bucket 0 perplexity 76.52
  eval: bucket 1 perplexity 125.97
  eval: bucket 2 perplexity 150.13
  eval: bucket 3 perplexity 181.07
...
</pre></div>
</div>
<p>After training the model for 350000 steps, you can play with the translation by switch
<code class="docutils literal"><span class="pre">train()</span></code> to <code class="docutils literal"><span class="pre">decode()</span></code>. You type in a English sentence, the program will outputs
a French sentence.</p>
<div class="highlight-text"><div class="highlight"><pre><span></span>Reading model parameters from wmt/translate.ckpt-340000
&gt;  Who is the president of the United States?
Qui est le président des États-Unis ?
</pre></div>
</div>
</div>
<div class="section" id="understand-translation">
<h2>Understand Translation<a class="headerlink" href="#understand-translation" title="Permalink to this headline">¶</a></h2>
<div class="section" id="seq2seq">
<h3>Seq2seq<a class="headerlink" href="#seq2seq" title="Permalink to this headline">¶</a></h3>
<div class="section" id="basics">
<h4>Basics<a class="headerlink" href="#basics" title="Permalink to this headline">¶</a></h4>
<p>Sequence to sequence is a type of &#8220;Many to many&#8221; but different with Synced
sequence input and output in PTB tutorial. Seq2seq generates sequence output
after feeding all sequence inputs. The following two methods can improve the
accuracy:</p>
<blockquote>
<div><ul class="simple">
<li>Reversing the inputs</li>
<li>Attention mechanism</li>
</ul>
</div></blockquote>
<p>To speed up the computation, we used:</p>
<blockquote>
<div><ul class="simple">
<li>Sampled softmax</li>
</ul>
</div></blockquote>
<p>Karpathy&#8217;s blog described Seq2seq as:
&#8220;(4) Sequence input and sequence output (e.g. Machine Translation: an RNN
reads a sentence in English and then outputs a sentence in French).&#8221;</p>
<a class="reference internal image-reference" href="../_images/basic_seq2seq.png" id="id14"><img alt="../_images/basic_seq2seq.png" class="align-center" id="id14" src="../_images/basic_seq2seq.png" style="width: 1884.0px; height: 416.0px;" /></a>
<p>As the above figure shows, the encoder inputs, decoder inputs and targets are:</p>
<div class="highlight-text"><div class="highlight"><pre><span></span>encoder_input =  A    B    C
decoder_input =  &lt;go&gt; W    X    Y    Z
targets       =  W    X    Y    Z    &lt;eos&gt;

Note: in the code, the size of targets is one smaller than the size
of decoder_input, not like this figure.
</pre></div>
</div>
</div>
<div class="section" id="papers">
<h4>Papers<a class="headerlink" href="#papers" title="Permalink to this headline">¶</a></h4>
<p>The English-to-French example implements a multi-layer recurrent neural
network as encoder, and an Attention-based decoder.
It is the same as the model described in this paper:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference external" href="http://arxiv.org/abs/1412.7449">Grammar as a Foreign Language</a></li>
</ul>
</div></blockquote>
<p>The example uses sampled softmax to handle large output vocabulary size.
In this case, as <code class="docutils literal"><span class="pre">target_vocab_size=4000</span></code>, for vocabularies smaller
than <code class="docutils literal"><span class="pre">512</span></code>, it might be a better idea to just use a standard softmax loss.
Sampled softmax is described in Section 3 of the this paper:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference external" href="http://arxiv.org/abs/1412.2007">On Using Very Large Target Vocabulary for Neural Machine Translation</a></li>
</ul>
</div></blockquote>
<p>Reversing the inputs and Multi-layer cells have been successfully used in
sequence-to-sequence models for translation has beed described in this paper:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference external" href="http://arxiv.org/abs/1409.3215">Sequence to Sequence Learning with Neural Networks</a></li>
</ul>
</div></blockquote>
<p>Attention mechanism allows the decoder more direct access to the input, it was
described in this paper:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference external" href="http://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a></li>
</ul>
</div></blockquote>
<p>Alternatively, the model can also be implemented by a single-layer
version, but with Bi-directional encoder, was presented in this paper:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference external" href="http://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a></li>
</ul>
</div></blockquote>
</div>
</div>
<div class="section" id="implementation">
<h3>Implementation<a class="headerlink" href="#implementation" title="Permalink to this headline">¶</a></h3>
<div class="section" id="bucketing-and-padding">
<h4>Bucketing and Padding<a class="headerlink" href="#bucketing-and-padding" title="Permalink to this headline">¶</a></h4>
<p>Bucketing is a method to efficiently handle sentences of different length.
When translating English to French, we will have English sentences of
different lengths <code class="docutils literal"><span class="pre">L1</span></code> on input, and French sentences of different
lengths <code class="docutils literal"><span class="pre">L2</span></code> on output. We should in principle create a seq2seq model
for every pair <code class="docutils literal"><span class="pre">(L1,</span> <span class="pre">L2+1)</span></code> (prefixed by a GO symbol) of
lengths of an English and French sentence.</p>
<p>For find the closest bucket for each pair, then we could just pad every
sentence with a special PAD symbol in the end if the bucket is bigger
than the sentence</p>
<p>We use a number of buckets and pad to the closest one for efficiency.
In this example, we used 4 buckets.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">buckets</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">),</span> <span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">25</span><span class="p">),</span> <span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="mi">50</span><span class="p">)]</span>
</pre></div>
</div>
<p>If the input is an English sentence with <code class="docutils literal"><span class="pre">3</span></code> tokens, and the corresponding
output is a French sentence with <code class="docutils literal"><span class="pre">6</span></code> tokens, then they will be put in the
first bucket and padded to length <code class="docutils literal"><span class="pre">5</span></code> for encoder inputs (English sentence),
and length <code class="docutils literal"><span class="pre">10</span></code> for decoder inputs.
If we have an English sentence with 8 tokens and the corresponding French
sentence has 18 tokens, then they will be fit into <code class="docutils literal"><span class="pre">(20,</span> <span class="pre">25)</span></code> bucket.</p>
<p>In other word, bucket <code class="docutils literal"><span class="pre">(I,</span> <span class="pre">O)</span></code> is <code class="docutils literal"><span class="pre">(encoder_input_size,</span> <span class="pre">decoder_inputs_size)</span></code>.</p>
<p>Given a pair of <code class="docutils literal"><span class="pre">[[&quot;I&quot;,</span> <span class="pre">&quot;go&quot;,</span> <span class="pre">&quot;.&quot;],</span> <span class="pre">[&quot;Je&quot;,</span> <span class="pre">&quot;vais&quot;,</span> <span class="pre">&quot;.&quot;]]</span></code> in tokenized format,
we fit it into bucket <code class="docutils literal"><span class="pre">(5,</span> <span class="pre">10)</span></code>.
The training data of encoder inputs representing <code class="docutils literal"><span class="pre">[PAD</span> <span class="pre">PAD</span> <span class="pre">&quot;.&quot;</span> <span class="pre">&quot;go&quot;</span> <span class="pre">&quot;I&quot;]</span></code>
and decoder inputs <code class="docutils literal"><span class="pre">[GO</span> <span class="pre">&quot;Je&quot;</span> <span class="pre">&quot;vais&quot;</span> <span class="pre">&quot;.&quot;</span> <span class="pre">EOS</span> <span class="pre">PAD</span> <span class="pre">PAD</span> <span class="pre">PAD</span> <span class="pre">PAD</span> <span class="pre">PAD]</span></code>. The targets
are decoder inputs shifted by one. The <code class="docutils literal"><span class="pre">target_weights</span></code> is the mask of
<code class="docutils literal"><span class="pre">targets</span></code>.</p>
<div class="highlight-text"><div class="highlight"><pre><span></span>bucket = (I, O) = (5, 10)
encoder_inputs = [PAD PAD &quot;.&quot; &quot;go&quot; &quot;I&quot;]                       &lt;-- 5  x batch_size
decoder_inputs = [GO &quot;Je&quot; &quot;vais&quot; &quot;.&quot; EOS PAD PAD PAD PAD PAD] &lt;-- 10 x batch_size
target_weights = [1   1     1     1   0 0 0 0 0 0 0]          &lt;-- 10 x batch_size
targets        = [&quot;Je&quot; &quot;vais&quot; &quot;.&quot; EOS PAD PAD PAD PAD PAD]    &lt;-- 9  x batch_size
</pre></div>
</div>
<p>In this script, one sentence is represented by one column, so assume
<code class="docutils literal"><span class="pre">batch_size</span> <span class="pre">=</span> <span class="pre">3</span></code>, <code class="docutils literal"><span class="pre">bucket</span> <span class="pre">=</span> <span class="pre">(5,</span> <span class="pre">10)</span></code> the training data will look like:</p>
<div class="highlight-text"><div class="highlight"><pre><span></span>encoder_inputs    decoder_inputs    target_weights    targets
0    0    0       1    1    1       1    1    1       87   71   16748
0    0    0       87   71   16748   1    1    1       2    3    14195
0    0    0       2    3    14195   0    1    1       0    2    2
0    0    3233    0    2    2       0    0    0       0    0    0
3    698  4061    0    0    0       0    0    0       0    0    0
                  0    0    0       0    0    0       0    0    0
                  0    0    0       0    0    0       0    0    0
                  0    0    0       0    0    0       0    0    0
                  0    0    0       0    0    0       0    0    0
                  0    0    0       0    0    0

where 0 : _PAD    1 : _GO     2 : _EOS      3 : _UNK
</pre></div>
</div>
<p>During training,</p>
<p>During prediction,</p>
</div>
<div class="section" id="special-vocabulary-symbols-punctuations-and-digits">
<h4>Special vocabulary symbols, punctuations and digits<a class="headerlink" href="#special-vocabulary-symbols-punctuations-and-digits" title="Permalink to this headline">¶</a></h4>
<p>The special vocabulary symbols in this example are:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">_PAD</span> <span class="o">=</span> <span class="n">b</span><span class="s2">&quot;_PAD&quot;</span>
<span class="n">_GO</span> <span class="o">=</span> <span class="n">b</span><span class="s2">&quot;_GO&quot;</span>
<span class="n">_EOS</span> <span class="o">=</span> <span class="n">b</span><span class="s2">&quot;_EOS&quot;</span>
<span class="n">_UNK</span> <span class="o">=</span> <span class="n">b</span><span class="s2">&quot;_UNK&quot;</span>
<span class="n">PAD_ID</span> <span class="o">=</span> <span class="mi">0</span>      <span class="o">&lt;--</span> <span class="n">index</span> <span class="p">(</span><span class="n">row</span> <span class="n">number</span><span class="p">)</span> <span class="ow">in</span> <span class="n">vocabulary</span>
<span class="n">GO_ID</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">EOS_ID</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">UNK_ID</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">_START_VOCAB</span> <span class="o">=</span> <span class="p">[</span><span class="n">_PAD</span><span class="p">,</span> <span class="n">_GO</span><span class="p">,</span> <span class="n">_EOS</span><span class="p">,</span> <span class="n">_UNK</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-text"><div class="highlight"><pre><span></span>        ID    MEANINGS
_PAD    0     Padding, empty word
_GO     1     1st element of decoder_inputs
_EOS    2     End of Sentence of targets
_UNK    3     Unknown word, words do not exist in vocabulary will be marked as 3
</pre></div>
</div>
<p>For digits, the <code class="docutils literal"><span class="pre">normalize_digits</span></code> of creating vocabularies and tokenized dataset
must be consistent, if <code class="docutils literal"><span class="pre">True</span></code> all digits will be replaced by <code class="docutils literal"><span class="pre">0</span></code>. Like
<code class="docutils literal"><span class="pre">123</span></code> to <code class="docutils literal"><span class="pre">000`</span></code>, <cite>9</cite> to <cite>0</cite> and <cite>1990-05</cite> to <cite>0000-00</cite>, then <cite>000</cite>, <cite>0</cite> and
<cite>0000-00</cite> etc will be the words in the vocabulary (see <code class="docutils literal"><span class="pre">vocab40000.en</span></code>).
Otherwise, if <code class="docutils literal"><span class="pre">False</span></code>, different digits
will be seem in the vocabulary, then the vocabulary size will be very big.
The regular expression to find digits is <code class="docutils literal"><span class="pre">_DIGIT_RE</span> <span class="pre">=</span> <span class="pre">re.compile(br&quot;\d&quot;)</span></code>.
(see <code class="docutils literal"><span class="pre">tl.nlp.create_vocabulary()</span></code> and <code class="docutils literal"><span class="pre">tl.nlp.data_to_token_ids()</span></code>)</p>
<p>For word split, the regular expression is
<code class="docutils literal"><span class="pre">_WORD_SPLIT</span> <span class="pre">=</span> <span class="pre">re.compile(b&quot;([.,!?\&quot;':;)(])&quot;)</span></code>, this means use
<code class="docutils literal"><span class="pre">[</span> <span class="pre">.</span> <span class="pre">,</span> <span class="pre">!</span> <span class="pre">?</span> <span class="pre">&quot;</span> <span class="pre">'</span> <span class="pre">:</span> <span class="pre">;</span> <span class="pre">)</span> <span class="pre">(</span> <span class="pre">]</span></code> and space to split the sentence, see
<code class="docutils literal"><span class="pre">tl.nlp.basic_tokenizer()</span></code> which is the default tokenizer of
<code class="docutils literal"><span class="pre">tl.nlp.create_vocabulary()</span></code> and <code class="docutils literal"><span class="pre">tl.nlp.data_to_token_ids()</span></code>.</p>
<p>All punctuation marks, such as <code class="docutils literal"><span class="pre">.</span> <span class="pre">,</span> <span class="pre">)</span> <span class="pre">(</span></code> are all reserved in the vocabularies
of both English and French.</p>
</div>
<div class="section" id="sampled-softmax">
<h4>Sampled softmax<a class="headerlink" href="#sampled-softmax" title="Permalink to this headline">¶</a></h4>
</div>
<div class="section" id="id16">
<h4>Dataset iteration<a class="headerlink" href="#id16" title="Permalink to this headline">¶</a></h4>
</div>
<div class="section" id="id17">
<h4>Loss and update expressions<a class="headerlink" href="#id17" title="Permalink to this headline">¶</a></h4>
</div>
</div>
<div class="section" id="id18">
<h3>What Next?<a class="headerlink" href="#id18" title="Permalink to this headline">¶</a></h3>
</div>
</div>
<div class="section" id="cost-functions">
<h2>Cost Functions<a class="headerlink" href="#cost-functions" title="Permalink to this headline">¶</a></h2>
<p>TuneLayer provides a simple way to creat you own cost function. Take a MLP below for example.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">InputLayer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;input_layer&#39;</span><span class="p">)</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">DropoutLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">keep</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;drop1&#39;</span><span class="p">)</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">n_units</span><span class="o">=</span><span class="mi">800</span><span class="p">,</span> <span class="n">act</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;relu1&#39;</span><span class="p">)</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">DropoutLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">keep</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;drop2&#39;</span><span class="p">)</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">n_units</span><span class="o">=</span><span class="mi">800</span><span class="p">,</span> <span class="n">act</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;relu2&#39;</span><span class="p">)</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">DropoutLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">keep</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;drop3&#39;</span><span class="p">)</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">n_units</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">act</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">identity</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;output_layer&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="regularization-of-weights">
<h3>Regularization of Weights<a class="headerlink" href="#regularization-of-weights" title="Permalink to this headline">¶</a></h3>
<p>After initializing the variables, the informations of network parameters can be
observed by using <code class="docutils literal"><span class="pre">network.print_params()</span></code>.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">initialize_all_variables</span><span class="p">())</span>
<span class="n">network</span><span class="o">.</span><span class="n">print_params</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text"><div class="highlight"><pre><span></span>param 0: (784, 800) (mean: -0.000000, median: 0.000004 std: 0.035524)
param 1: (800,) (mean: 0.000000, median: 0.000000 std: 0.000000)
param 2: (800, 800) (mean: 0.000029, median: 0.000031 std: 0.035378)
param 3: (800,) (mean: 0.000000, median: 0.000000 std: 0.000000)
param 4: (800, 10) (mean: 0.000673, median: 0.000763 std: 0.049373)
param 5: (10,) (mean: 0.000000, median: 0.000000 std: 0.000000)
num of params: 1276810
</pre></div>
</div>
<p>The output of network is <code class="docutils literal"><span class="pre">network.outputs</span></code>, then the cross entropy can be
defined as follow. Besides, to regularize the weights,
the <code class="docutils literal"><span class="pre">network.all_params</span></code> contains all parameters of the network.
In this case, <code class="docutils literal"><span class="pre">network.all_params</span> <span class="pre">=</span> <span class="pre">[W1,</span> <span class="pre">b1,</span> <span class="pre">W2,</span> <span class="pre">b2,</span> <span class="pre">Wout,</span> <span class="pre">bout]</span></code> according
to param 0, 1 ... 5 shown by <code class="docutils literal"><span class="pre">network.print_params()</span></code>.
Then max-norm regularization on W1 and W2 can be performed as follow.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">outputs</span>
<span class="c1"># Alternatively, you can use tl.cost.cross_entropy(y, y_) instead.</span>
<span class="n">cross_entropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_</span><span class="p">))</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">cross_entropy</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">cost</span> <span class="o">+</span> <span class="n">tl</span><span class="o">.</span><span class="n">cost</span><span class="o">.</span><span class="n">maxnorm_regularizer</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)(</span><span class="n">network</span><span class="o">.</span><span class="n">all_params</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">+</span>
          <span class="n">tl</span><span class="o">.</span><span class="n">cost</span><span class="o">.</span><span class="n">maxnorm_regularizer</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)(</span><span class="n">network</span><span class="o">.</span><span class="n">all_params</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
</pre></div>
</div>
<p>In addition, all TensorFlow&#8217;s regularizers like
<code class="docutils literal"><span class="pre">tf.contrib.layers.l2_regularizer</span></code> can be used with TuneLayer.</p>
</div>
<div class="section" id="regularization-of-activation-outputs">
<h3>Regularization of Activation outputs<a class="headerlink" href="#regularization-of-activation-outputs" title="Permalink to this headline">¶</a></h3>
<p>Instance method <code class="docutils literal"><span class="pre">network.print_layers()</span></code> prints all outputs of different
layers in order. To achieve regularization on activation output, you can use
<code class="docutils literal"><span class="pre">network.all_layers</span></code> which contains all outputs of different layers.
If you want to apply L1 penalty on the activations of first hidden layer,
just simply add <code class="docutils literal"><span class="pre">tf.contrib.layers.l2_regularizer(lambda_l1)(network.all_layers[1])</span></code>
to the cost function.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">network</span><span class="o">.</span><span class="n">print_layers</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text"><div class="highlight"><pre><span></span>layer 0: Tensor(&quot;dropout/mul_1:0&quot;, shape=(?, 784), dtype=float32)
layer 1: Tensor(&quot;Relu:0&quot;, shape=(?, 800), dtype=float32)
layer 2: Tensor(&quot;dropout_1/mul_1:0&quot;, shape=(?, 800), dtype=float32)
layer 3: Tensor(&quot;Relu_1:0&quot;, shape=(?, 800), dtype=float32)
layer 4: Tensor(&quot;dropout_2/mul_1:0&quot;, shape=(?, 800), dtype=float32)
layer 5: Tensor(&quot;add_2:0&quot;, shape=(?, 10), dtype=float32)
</pre></div>
</div>
</div>
</div>
<div class="section" id="easy-to-modify">
<h2>Easy to Modify<a class="headerlink" href="#easy-to-modify" title="Permalink to this headline">¶</a></h2>
<div class="section" id="modifying-pre-train-behaviour">
<h3>Modifying Pre-train Behaviour<a class="headerlink" href="#modifying-pre-train-behaviour" title="Permalink to this headline">¶</a></h3>
<p>Greedy layer-wise pretrain is an important task for deep neural network
initialization, while there are many kinds of pre-train metrics according
to different architectures and applications.</p>
<p>For example, the pre-train process of <a class="reference external" href="http://deeplearning.stanford.edu/wiki/index.php/Autoencoders_and_Sparsity">Vanilla Sparse Autoencoder</a>
can be implemented by using KL divergence as the following code,
but for <a class="reference external" href="http://www.jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf">Deep Rectifier Network</a>,
the sparsity can be implemented by using the L1 regularization of activation output.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1"># Vanilla Sparse Autoencoder</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">rho</span> <span class="o">=</span> <span class="mf">0.15</span>
<span class="n">p_hat</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">activation_out</span><span class="p">,</span> <span class="n">reduction_indices</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">KLD</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span> <span class="n">rho</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">rho</span><span class="p">,</span> <span class="n">p_hat</span><span class="p">))</span>
        <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span> <span class="n">rho</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">log</span><span class="p">((</span><span class="mi">1</span><span class="o">-</span> <span class="n">rho</span><span class="p">)</span><span class="o">/</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">p_hat</span><span class="p">)))</span> <span class="p">)</span>
</pre></div>
</div>
<p>For this reason, TuneLayer provides a simple way to modify or design your
own pre-train metrice. For Autoencoder, TuneLayer uses <code class="docutils literal"><span class="pre">ReconLayer.__init__()</span></code>
to define the reconstruction layer and cost function, to define your own cost
function, just simply modify the <code class="docutils literal"><span class="pre">self.cost</span></code> in <code class="docutils literal"><span class="pre">ReconLayer.__init__()</span></code>.
To creat your own cost expression please read <a class="reference external" href="https://www.tensorflow.org/versions/master/api_docs/python/math_ops.html">Tensorflow Math</a>.
By default, <code class="docutils literal"><span class="pre">ReconLayer</span></code> only updates the weights and biases of previous 1
layer by using <code class="docutils literal"><span class="pre">self.train_params</span> <span class="pre">=</span> <span class="pre">self.all</span> <span class="pre">_params[-4:]</span></code>, where the 4
parameters are <code class="docutils literal"><span class="pre">[W_encoder,</span> <span class="pre">b_encoder,</span> <span class="pre">W_decoder,</span> <span class="pre">b_decoder]</span></code>. If you want
to update the parameters of previous 2 layers, simply modify <code class="docutils literal"><span class="pre">[-4:]</span></code> to <code class="docutils literal"><span class="pre">[-6:]</span></code>.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">ReconLayer</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">train_params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_params</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">:]</span>
    <span class="o">...</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">cost</span> <span class="o">=</span> <span class="n">mse</span> <span class="o">+</span> <span class="n">L1_a</span> <span class="o">+</span> <span class="n">L2_w</span>
</pre></div>
</div>
</div>
<div class="section" id="adding-customized-layer">
<h3>Adding Customized Layer<a class="headerlink" href="#adding-customized-layer" title="Permalink to this headline">¶</a></h3>
<p>Contribute useful <code class="docutils literal"><span class="pre">Layer</span></code> as an developer. The source code of TuneLayer is
easy to understand, open <code class="docutils literal"><span class="pre">tunelayer/layer.py</span></code> and read <code class="docutils literal"><span class="pre">DenseLayer</span></code>, you
can fully understand how it work.</p>
</div>
<div class="section" id="adding-customized-regularizer">
<h3>Adding Customized Regularizer<a class="headerlink" href="#adding-customized-regularizer" title="Permalink to this headline">¶</a></h3>
<p>See tunelayer/cost.py</p>
</div>
</div>
<div class="section" id="more-info">
<h2>More info<a class="headerlink" href="#more-info" title="Permalink to this headline">¶</a></h2>
<p>For more information on what you can do with TuneLayer, just continue
reading through readthedocs.
Finally, the reference lists and explains as follow.</p>
<p>layers (<code class="xref py py-mod docutils literal"><span class="pre">tunelayer.layers</span></code>),</p>
<p>activation (<code class="xref py py-mod docutils literal"><span class="pre">tunelayer.activation</span></code>),</p>
<p>natural language processing (<code class="xref py py-mod docutils literal"><span class="pre">tunelayer.nlp</span></code>),</p>
<p>reinforcement learning (<code class="xref py py-mod docutils literal"><span class="pre">tunelayer.rein</span></code>),</p>
<p>cost expressions and regularizers (<code class="xref py py-mod docutils literal"><span class="pre">tunelayer.cost</span></code>),</p>
<p>load and save files (<code class="xref py py-mod docutils literal"><span class="pre">tunelayer.files</span></code>),</p>
<p>operating system (<code class="xref py py-mod docutils literal"><span class="pre">tunelayer.ops</span></code>),</p>
<p>helper functions (<code class="xref py py-mod docutils literal"><span class="pre">tunelayer.utils</span></code>),</p>
<p>visualization (<code class="xref py py-mod docutils literal"><span class="pre">tunelayer.visualize</span></code>),</p>
<p>iteration functions (<code class="xref py py-mod docutils literal"><span class="pre">tunelayer.iterate</span></code>),</p>
<p>preprocessing functions (<code class="xref py py-mod docutils literal"><span class="pre">tunelayer.preprocess</span></code>),</p>
</div>
</div>


           </div>
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2016, Hao Dong.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'1.1',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>